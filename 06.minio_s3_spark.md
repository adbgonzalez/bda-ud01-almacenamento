# Uso de MinIO como almacenamento S3 con Spark

## 0. Introdución a Apache Spark
**Apache Spark** é un motor distribuído para o procesamento de datos a gran escala.
Permite executar tarefas de análise, transformación ou aprendizaxe automática sobre grandes volumes de datos en paralelo, repartidos entre múltiples nodos dun clúster.

No noso entorno (con **Hadoop** + **YARN** + **MinIO**) Spark actúa como capa de **procesamento**, mentres que Hadoop (HDFS ou MinIO via S3A) serve como **almacenamento**: [Repositorio spark](https://github.com/adbgonzalez/spark-cluster-yarn)

### Crear unha Spark Session
A *SparkSession* é o punto de entrada a todas as funcións de Spark.
```python
import findspark
findspark.init("/opt/spark")

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("nome-app")
    .master("yarn")
    .config("spark.eventLog.enabled", "true")
    .config("spark.eventLog.dir", "hdfs://namenode:9000/spark-logs")
    .getOrCreate()
)

spark.range(1, 1000000).selectExpr("sum(id)").show()
```

### Lectura de datos
Spark pode ler ficheiros en diferentes formatos de almacenamento. Estes son es métodos que permiten a lectura dos mesmos:
| Formato         | Método de lectura                                             | Opcións comúns                               | Notas                                                                                      |
| --------------- | ------------------------------------------------------------- | -------------------------------------------- | ------------------------------------------------------------------------------------------ |
| **CSV**         | `spark.read.csv(path, ...)`                                   | `header=True`, `inferSchema=True`, `sep=","` | Formato de texto tabular, moi usado para datos brutos (*bronze*).                          |
| **JSON**        | `spark.read.json(path)`                                       | `multiLine=True` (para JSONs grandes)        | Ideal para datos semiestruturados.                                                         |
| **Parquet**     | `spark.read.parquet(path)`                                    | —                                            | Formato columnar optimizado (compresión e lectura rápida). Recomendado para *silver/gold*. |
| **ORC**         | `spark.read.orc(path)`                                        | —                                            | Similar a Parquet, moi eficiente en clústeres Hadoop.                                      |
| **Avro**        | `spark.read.format("avro").load(path)`                        | —                                            | Formato binario estruturado, útil en fluxos Kafka ou integracións con Hive.                |
| **Text**        | `spark.read.text(path)`                                       | —                                            | Lectura liña a liña de ficheiros de texto plano.                                           |
| **Delta Lake**  | `spark.read.format("delta").load(path)`                       | —                                            | Engade control de versións e transaccións ACID sobre Parquet.                              |
| **HDFS**        | `spark.read.parquet("hdfs:///ruta")`                          | —                                            | Igual que Parquet, pero indicando protocolo HDFS.                                          |
| **S3 / MinIO**  | `spark.read.parquet("s3a://bucket/ruta")`                     | —                                            | Acceso vía conector `s3a`. Require configuración `fs.s3a.*`.                               |
| **JDBC**        | `spark.read.format("jdbc").options(...).load()`               | `url`, `dbtable`, `user`, `password`         | Para conexións con bases de datos SQL.                                                     |
| **Hive tables** | `spark.read.table("nome_tabla")` ou `spark.sql("SELECT ...")` | —                                            | Se Spark está integrado con Hive Metastore.                                                |
| **Binary**      | `spark.read.format("binaryFile").load(path)`                  | `recursiveFileLookup=True`                   | Para ler imaxes ou ficheiros binarios en bruto.                                            |

Exemplos:
```python
# CSV
df_csv = spark.read.csv("s3a://bronze/datos.csv", header=True, inferSchema=True)

# JSON
df_json = spark.read.json("s3a://bronze/datos.json")

# Parquet
df_parquet = spark.read.parquet("s3a://silver/datos/")

# Avro
df_avro = spark.read.format("avro").load("s3a://bronze/datos.avro")

# JDBC
df_jdbc = (spark.read.format("jdbc")
           .option("url", "jdbc:postgresql://db:5432/empresa")
           .option("dbtable", "clientes")
           .option("user", "postgres")
           .option("password", "admin")
           .load())
```
### Escritura de datos
A escritura realízase de forma moi semellante:
| Formato         | Método de escritura                           | Opcións comúns                               | Notas                                                                       |
| --------------- | --------------------------------------------- | -------------------------------------------- | --------------------------------------------------------------------------- |
| **CSV**         | `df.write.csv(path)`                          | `header=True`, `sep=","`, `mode="overwrite"` | Ideal para exportar datos en formato lixeiro; non conserva tipos complexos. |
| **JSON**        | `df.write.json(path)`                         | `mode="overwrite"`, `compression="gzip"`     | Bo para datos semiestruturados, readable por humanos.                       |
| **Parquet**     | `df.write.parquet(path)`                      | `mode="overwrite"`, `compression="snappy"`   | Formato columnar óptimo; recomendado para *silver* e *gold*.                |
| **ORC**         | `df.write.orc(path)`                          | `mode="overwrite"`                           | Similar a Parquet, moi eficiente en Hadoop.                                 |
| **Avro**        | `df.write.format("avro").save(path)`          | `mode="overwrite"`                           | Bo para integracións con Kafka ou sistemas baseados en esquemas.            |
| **Text**        | `df.write.text(path)`                         | `mode="overwrite"`                           | Escribe unha liña por rexistro; útil para logs.                             |
| **Delta Lake**  | `df.write.format("delta").save(path)`         | `mode="overwrite"`                           | Engade control de versións e ACID a Parquet.                                |
| **HDFS**        | `df.write.parquet("hdfs:///ruta")`            | —                                            | Igual que Parquet, indicando protocolo HDFS.                                |
| **S3 / MinIO**  | `df.write.parquet("s3a://bucket/ruta")`       | —                                            | Usa o conector `s3a` para escribir en MinIO/AWS S3.                         |
| **JDBC**        | `df.write.format("jdbc").options(...).save()` | `url`, `dbtable`, `user`, `password`, `mode` | Para gardar resultados en bases de datos SQL.                               |
| **Hive tables** | `df.write.saveAsTable("nome_tabla")`          | `mode="overwrite"`                           | Garda directamente no Hive Metastore.                                       |
| **Binary**      | `df.write.format("binaryFile").save(path)`    | —                                            | Escribe ficheiros binarios (pouco común, só casos específicos).             |

Exemplos:
```python
# CSV
df.write.mode("overwrite").option("header", True).csv("s3a://silver/iris_csv/")

# JSON
df.write.mode("overwrite").json("s3a://gold/iris_json/")

# Parquet
df.write.mode("overwrite").parquet("s3a://gold/iris/")

# Avro
df.write.mode("overwrite").format("avro").save("s3a://gold/iris_avro/")

# JDBC
(df.write.format("jdbc")
   .option("url", "jdbc:postgresql://db:5432/empresa")
   .option("dbtable", "clientes_resultado")
   .option("user", "postgres")
   .option("password", "admin")
   .mode("overwrite")
   .save())

```
### Resumo
| Acción           | Método                               | Exemplo                                                                  |
| ---------------- | ------------------------------------ | ------------------------------------------------------------------------ |
| Crear sesión     | `SparkSession.builder.getOrCreate()` | `spark = SparkSession.builder.appName("demo").getOrCreate()`             |
| Ler CSV          | `spark.read.csv()`                   | `spark.read.csv("s3a://bronze/iris.csv", header=True, inferSchema=True)` |
| Ler Parquet      | `spark.read.parquet()`               | `spark.read.parquet("s3a://silver/iris/")`                               |
| Escribir CSV     | `df.write.csv()`                     | `df.write.mode("overwrite").csv("s3a://gold/iris_csv/")`                 |
| Escribir Parquet | `df.write.parquet()`                 | `df.write.mode("overwrite").parquet("s3a://gold/iris/")`                 |

## 1. Que é MinIO?

**MinIO** é unha solución de almacenamento de obxectos **compatible co protocolo S3** de Amazon Web Services. Está deseñada para ser **lixeira, rápida e autoaloxada**, ideal para laboratorios e contornos Big Data locais.

En lugar de gardar ficheiros nun sistema de ficheiros clásico (HDFS, ext4, etc.), MinIO organiza os datos en:
- **Buckets** (equivalentes a “directorios raíz”)
- **Obxectos** (ficheiros binarios ou texto)
- **Metadatos** (información sobre permisos, tamaño, tipo, etc.)

---

## 2. Arquitectura básica

Nun clúster como o teu, MinIO funciona como **servizo adicional** dentro da rede Docker (`network_cluster`), ofrecendo unha API S3 accesible dende Spark, Hadoop ou outras aplicacións.

```
[YARN / Hadoop]
  - NameNode + DataNodes
  - ResourceManager + NodeManagers
           |                 \ 
           |                  \__ [Spark History Server]  (ler logs)
           |                        ^
[ spark-notebook ]  ---- usar S3A ---| 
           |                          \
           +------------------------> [MinIO Server] <--> [mc]
                                        (S3 API)

```

- Porto **9000** → API S3 (para Spark, Hadoop, `mc`, etc.)
- Porto **9001** → consola web (para xestionar buckets e obxectos)

---

## 3. Engadir MinIO ao `docker-compose`

Engade ao teu ficheiro `docker-compose.yml` o seguinte bloque (axustando portos se xa usas o 9000):

```yaml
services:
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9001:9001"   # consola
      - "9002:9000"   # API interna 9000 → publicada como 9002 no host
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - hadoop
```

> **Acceso web:** [http://localhost:9001](http://localhost:9001)  
> **Usuario:** `minioadmin`  
> **Contrasinal:** `minioadmin`

---

## 4. Configuración en Spark (S3A Connector)

Para que Spark poida ler e escribir en MinIO, hai que engadir o **conector S3A** e a súa configuración.

### 4.1. Engadir as JARs necesarias

Copia estes dous ficheiros ao cartafol `./jars` e móntao en todos os servizos Spark:

- [`hadoop-aws-3.3.6.jar`](https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.3.6)
- [`aws-java-sdk-bundle-1.12.x.jar`](https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-bundle/1.12.262)
No `docker-compose-with-minio.yml` hai que montar este volume en todos os servizos de spark.
```yaml
volumes:
  - ./jars:/opt/spark/jars-extra:ro
```
No `sparks-default.conf` é necesario engadir esta liña:
```bash
#Spark jars
spark.jars /opt/spark/jars-extra/hadoop-aws-3.3.6.jar,/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.262.jar

```


### 4.2. Engadir configuración en `spark-defaults.conf` ou variables de contorno

#### Opción 1: variables de contorno (sinxela para Jupyter)
```yaml
environment:
  - SPARK_MASTER_URL=spark://spark-master:7077
  - SPARK_EVENTLOG_ENABLED=true
  - SPARK_EVENTLOG_DIR=hdfs:///spark/logs/history
  - SPARK_EXTRA_CONF=\
      spark.hadoop.fs.s3a.access.key=minio,\
      spark.hadoop.fs.s3a.secret.key=minio12345,\
      spark.hadoop.fs.s3a.endpoint=http://minio:9000,\
      spark.hadoop.fs.s3a.path.style.access=true,\
      spark.hadoop.fs.s3a.connection.ssl.enabled=false,\
      spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
```

#### Opción 2: editar `/opt/spark/conf/spark-defaults.conf` (RECOMENDADA)

```
# S3A / MinIO
spark.hadoop.fs.s3a.endpoint              http://minio:9000
spark.hadoop.fs.s3a.access.key            minioadmin
spark.hadoop.fs.s3a.secret.key            minioadmin
spark.hadoop.fs.s3a.path.style.access     true
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.impl                  org.apache.hadoop.fs.s3a.S3AFileSystem
```

---
## 5. Creación e xestión de buckets en MinIO

### Introdución teórica

En MinIO, igual que en Amazon S3, o **bucket** é a unidade lóxica principal de almacenamento.  
Cada bucket actúa como un *contenedor* onde se gardan os obxectos (ficheiros ou blobs), e pode configurarse con políticas de acceso, versións, cifrado e outros parámetros.

Pódese pensar nun bucket como o equivalente a un **“directorio raíz”**, pero cunha diferenza clave:  
mentres que un sistema de ficheiros tradicional almacena datos en bloques nunha árbore de directorios, un sistema de obxectos como MinIO usa unha estrutura **plana**, onde cada obxecto está identificado por unha **chave única** (key).

---

### Estrutura conceptual dun bucket

Un bucket almacena obxectos cun formato de ruta tipo:

```
s3a://nome_do_bucket/ruta/ata/o/ficheiro.csv
```

Cada obxecto ten:
- **Key:** a ruta completa (“datasets/2025/temperaturas.csv”)
- **Valor:** o contido binario
- **Metadatos:** información adicional (data de creación, tamaño, tipo MIME, versión...)

En MinIO, a clave (“key”) pódese usar para simular subdirectorios, pero realmente non existen carpetas:  
é só un prefixo lexicográfico.

---

### Exemplo visual

Se creas tres obxectos cos nomes seguintes:

```
s3a://datasets/2025/enero.csv
s3a://datasets/2025/febrero.csv
s3a://datasets/2024/enero.csv
```

No navegador web de MinIO parecerán directorios (`2024`, `2025`), pero internamente son só *keys*:

| Key | Tamaño | Última modificación |
|-----|---------|---------------------|
| 2024/enero.csv | 12 KB | 2025-01-01 |
| 2025/enero.csv | 14 KB | 2025-02-01 |
| 2025/febrero.csv | 13 KB | 2025-03-01 |

---

### Ferramentas para xestionar buckets

Podes xestionar MinIO de tres formas principais:

1. **Consola web** (en `http://localhost:9001`)  
   Ideal para tarefas sinxelas: crear buckets, subir ficheiros, ver obxectos, establecer políticas públicas/privadas.

2. **CLI oficial `mc` (MinIO Client)**  
   Ferramenta de liña de comandos potente e automatizable.  
   A súa sintaxe lembra a de `aws cli` de Amazon.

3. **API S3 / SDKs**  
   Usada por aplicacións e frameworks (por exemplo, Spark, Hadoop, Airflow, etc.).

---

### Uso práctico do cliente `mc`

#### 1. Crear un contedor interactivo de `mc`

Se non tes o cliente instalado localmente, podes usalo directamente dende un contedor Docker:

```bash
docker run --network spark-cluster2526_hadoop --rm -it --entrypoint=/bin/sh minio/mc    
```

Explicación:
- `--network spark-cluster2526_hadoop`: conecta co mesmo *bridge network* que MinIO e Spark.
- `--rm`: elimina o contedor ao saír.
- `-it`: modo interactivo.
- `--entrypoint=/bin/sh`: iniciamos o shell.
- `minio/mc`: imaxe oficial do cliente MinIO.

---

#### 2 Definir un alias (punto de conexión)

Antes de facer calquera operación, tes que rexistrar o servidor ao que te vas conectar.

```bash
mc alias set local http://minio:9000 minioadmin minioadmin
```

Significado dos parámetros:

| Parámetro | Descrición |
|------------|-------------|
| `local` | Nome do alias (pode ser calquera identificador) |
| `http://minio:9000` | Endpoint do servidor MinIO (usa o nome do contedor) |
| `minioadmin` | Usuario raíz (MINIO_ROOT_USER) |
| `minioadmin` | Contrasinal (MINIO_ROOT_PASSWORD) |

Unha vez creado o alias, pódese usar “`local`” en lugar de escribir o endpoint completo.

Podes ver os alias rexistrados con:
```bash
mc alias list
```

---

#### 3️. Crear un novo bucket

```bash
mc mb local/spark
```

Explicación:
- `mb` → *make bucket*  
- `local` → alias definido antes  
- `spark` → nome do bucket

O nome do bucket debe cumprir certas regras:
- Só letras minúsculas, números e guións (`-`)
- Sen maiúsculas, nin espazos, nin guións baixos
- Debe ser único dentro da instalación

Exemplo de nomes válidos:
```
datasets, spark-logs, bronze, silver, gold
```

---

#### 4️. Listar buckets e obxectos

```bash
mc ls local
```
Mostra a lista de buckets dispoñibles.

```bash
mc ls local/spark
```
Mostra o contido do bucket `spark` (obxectos e prefixos).

Podes engadir `--recursive` para ver toda a estrutura.

---

#### 5️. Subir e descargar ficheiros

Subir un ficheiro:
```bash
mc cp datos.csv local/spark/
```

Descargar:
```bash
mc cp local/spark/datos.csv .
```

Subir directorios enteiros:
```bash
mc cp --recursive ./datasets local/spark/
```

O cliente `mc` calcula *hashes* (ETags) e reintenta automaticamente se hai cortes de conexión.

---

#### 6️. Eliminar buckets ou obxectos

Eliminar un obxecto:
```bash
mc rm local/spark/datos.csv
```

Eliminar un bucket completo:
```bash
mc rb --force local/spark
```

A opción `--force` borra todos os obxectos antes de eliminar o bucket.

---

#### 7. Referencia de comandos de `mc`

| Comando | Descrición | Exemplo |
|----------|-------------|----------|
| `mc alias set <nome> <endpoint> <usuario> <contrasinal>` | Crea un alias para conectar co servidor MinIO (ou outro S3). | `mc alias set local http://minio:9000 minioadmin minioadmin` |
| `mc alias list` | Lista todos os alias configurados. | `mc alias list` |
| `mc ls <alias>/<bucket>` | Lista buckets ou obxectos dun bucket. | `mc ls local/` ou `mc ls local/datos/` |
| `mc mb <alias>/<bucket>` | Crea un novo bucket. | `mc mb local/spark-logs` |
| `mc rb <alias>/<bucket>` | Elimina un bucket (debe estar baleiro). | `mc rb local/datos` |
| `mc rm <alias>/<bucket>/<obxecto>` | Elimina un ou varios ficheiros. | `mc rm local/datos/fichero.csv` |
| `mc cp <orixe> <destino>` | Copia ficheiros/buckets entre local e MinIO ou entre servidores. | `mc cp datos.csv local/datos/` |
| `mc mv <orixe> <destino>` | Move ficheiros ou obxectos. | `mc mv local/tmp/test.txt local/procesados/` |
| `mc cat <alias>/<bucket>/<obxecto>` | Mostra o contido dun ficheiro remoto. | `mc cat local/logs/app.log` |
| `mc find <alias>/<bucket> --name "*.parquet"` | Busca obxectos por nome, tamaño, etc. | `mc find local/data --name "*.csv"` |
| `mc du <alias>/<bucket>` | Mostra o uso de espazo dun bucket ou prefixo. | `mc du local/data` |
| `mc mirror <orixe> <destino>` | Sincroniza directorios ou buckets (bidireccional se se desexa). | `mc mirror ./datos local/backup` |
| `mc cp --recursive <orixe> <destino>` | Copia recursivamente directorios completos. | `mc cp --recursive local/input/ local/output/` |
| `mc admin info <alias>` | Mostra información do servidor (estado, versión, uso). | `mc admin info local` |
| `mc admin user add <alias> <usuario> <contrasinal>` | Engade un usuario novo en MinIO. | `mc admin user add local alumno pass1234` |
| `mc admin policy attach <alias> --user <usuario> <política>` | Asigna políticas (como `readwrite`, `readonly`, etc.) a un usuario. | `mc admin policy attach local readwrite --user alumno` |
| `mc share download <alias>/<bucket>/<obxecto>` | Xera un enlace temporal para descargar un obxecto. | `mc share download local/data/dataset.zip` |

##### Aliases comúns recomendados

| Alias | Endpoint | Uso típico |
|--------|-----------|------------|
| `local` | `http://minio:9000` | MinIO dentro do clúster Docker |
| `backup` | `http://minio-backup:9000` | Réplica ou entorno secundario |
| `aws` | `https://s3.amazonaws.com` | S3 público de AWS (compatibilidade) |

### Configuración avanzada de buckets

MinIO permite engadir políticas de acceso a cada bucket:

```bash
mc anonymous set public local/datasets
```
Fai que o bucket `datasets` sexa accesible publicamente.

Outras opcións:
```bash
mc anonymous set none local/datasets   # privado
mc anonymous set download local/docs   # só lectura
```

Tamén podes ver e editar políticas JSON:
```bash
mc admin policy info local readwrite
mc admin policy list local
```

---

### Exemplo completo paso a paso

```bash
# 1. Crear un alias
mc alias set local http://minio:9000 minio minio12345

# 2. Crear un bucket "datasets"
mc mb local/datasets

# 3. Subir ficheiros CSV e Parquet
mc cp /201508_station_data.csv local/datasets/
mc cp /2010-summary.parquet local/datasets/

# 4. Listar contido
mc ls local/datasets

# 5. Cambiar permisos para facelo público
mc anonymous set download local/datasets

# 6. Comprobar acceso dende navegador
# Navega a http://localhost:9000/datasets/temperaturas.csv
```

---

### Comparativa entre métodos de xestión

| Método | Vantaxes | Limitacións |
|--------|-----------|-------------|
| **Consola web** | Intuitiva, visual, ideal para demostracións | Limitada para automatización |
| **Cliente `mc`** | Potente, guións automáticos, control de permisos | Require coñecer comandos |
| **API / SDKs (Python, Java, etc.)** | Integración directa con aplicacións e frameworks | Máis complexidade de configuración |

---

### Conclusión

O cliente `mc` é a forma máis flexible e profesional de xestionar MinIO.  
Permite crear, listar, eliminar e configurar buckets de maneira reproducible, ideal para contornos docentes, laboratorios e integración con ferramentas de Big Data como **Apache Spark**.

Unha vez configurados os teus buckets, Spark pode acceder a eles directamente empregando rutas `s3a://`:

```python
df = spark.read.csv("s3a://datasets/temperaturas.csv", header=True)
df.show()
```

Deste xeito, MinIO convértese nun **almacenamento unificado** para experimentos, datos de produción ou proxectos multiusuario, funcionando coma un S3 privado local.

---

## 6. Uso básico con Spark

### 6.1. Escribir datos a MinIO
```python
df = spark.range(0, 10)
df.write.mode("overwrite").parquet("s3a://spark/demo_parquet")
```

### 6.2. Ler datos desde MinIO
```python
df2 = spark.read.parquet("s3a://spark/demo_parquet")
df2.show()
```

### 6.3. Comprobación de conexión
```python
spark._jsc.hadoopConfiguration().get("fs.s3a.endpoint")
```
Debe devolver `http://minio:9000`.

---

## 7. Exercicios propostos

### Exercicio 1: Verificar a conexión

1. Engade MinIO ao clúster e inícialo.  
2. Dende o Jupyter Notebook, executa:
   ```python
   spark._jsc.hadoopConfiguration().get("fs.s3a.endpoint")
   ```
3. Comproba que devolve o endpoint correcto.  
4. Proba un:
   ```python
   spark.range(5).write.csv("s3a://spark/test_csv")
   ```
   e verifica na consola web que se crearon ficheiros.

---

### Exercicio 2: Lectura e escritura de distintos formatos

1. Crea un bucket `datasets` e sube un CSV manualmente dende a consola web.  
2. Léeo con Spark:
   ```python
   df = spark.read.option("header", True).csv("s3a://datasets/iris.csv")
   df.show()
   ```
3. Escribe o mesmo DataFrame en formato Parquet:
   ```python
   df.write.mode("overwrite").parquet("s3a://datasets/iris_parquet")
   ```

---

### Exercicio 3: Uso combinado con HDFS

1. Escribe datos en HDFS:
   ```python
   df.write.mode("overwrite").parquet("hdfs:///tmp/hdfs_demo")
   ```
2. Copia o mesmo con:
   ```python
   df.write.mode("overwrite").parquet("s3a://spark/hdfs_copy")
   ```
3. Compara o tamaño de ambos directorios e observa as diferenzas.

---

### Exercicio 4: Configurar un *Data Lake* tipo Medallion

1. Crea tres buckets:
   - `bronze`
   - `silver`
   - `gold`
2. Define un fluxo:
   - Movemos `iris.csv`a `bronze`.
   - Ler datos de `bronze`, limpalos e escribir en silver (Parquet)
   ```python
      # partimos de df_bronze
      df_silver = (
         df_bronze
            .withColumnRenamed("sepal.length", "sepal_length")
            .withColumnRenamed("sepal.width",  "sepal_width")
            .withColumnRenamed("petal.length", "petal_length")
            .withColumnRenamed("petal.width",  "petal_width")  # petal_with → petal_width
      )  
   ```
   - Agregar e gardar en `gold` (Parquet consolidado)
   ```python
      from pyspark.sql import functions as F

      df_gold = (
         df_silver
            .groupBy("variety")
            .agg(
               F.avg("petal_length").alias("avg_petal_length"),
               F.avg("petal_width").alias("avg_petal_width")
            )
      )
   ```
3. Analiza tempos e tamaños resultantes.
   
---

## 8. Consellos e boas prácticas

- Emprega `path.style.access=true` sempre que uses MinIO local.  
- Desactiva SSL (`connection.ssl.enabled=false`) para contornos de laboratorio.  
- Crea buckets distintos para cada fase ou proxecto.  
- Adoita montar `minio_data` nun volume persistente.  
- Se usas MinIO nun contorno multiusuario, cambia as credenciais por variables seguras.

---

## 9. Recursos útiles

- [Documentación oficial de MinIO](https://min.io/docs/minio)  
- [Guía Hadoop S3A](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html)  
- [AWS SDK for Java](https://docs.aws.amazon.com/sdk-for-java/)  
- [Apache Spark – Cloud Integration](https://spark.apache.org/docs/latest/cloud-integration.html)
