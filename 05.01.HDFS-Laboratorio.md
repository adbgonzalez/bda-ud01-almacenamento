## Laboratorio guiado: Primeiros pasos con HDFS

### Obxectivo
Aprender a manexar o sistema de ficheiros distribuído **HDFS**, entendendo a súa estrutura, comandos básicos e ferramentas de administración.

---

### Requisitos previos
- Hadoop instalado 
- Variables de entorno configuradas:  
  ```bash
  export HADOOP_HOME=/usr/local/hadoop
  export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
  ```
- O servizo HDFS debe estar activo:  
  ```bash
  start-dfs.sh
  jps
  ```
  Deberías ver procesos como `NameNode`, `DataNode` e `SecondaryNameNode`.

---

### Paso 1: Crear a estrutura de directorios
Primeiro, imos crear unha pequena estrutura de directorios no HDFS.

```bash
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hadoop
hdfs dfs -mkdir /datos
```

**Comproba o resultado:**
```bash
hdfs dfs -ls /
```

---

### Paso 2: Subir ficheiros ao HDFS
Crea un ficheiro de proba no teu sistema local:

```bash
echo "Benvido a HDFS" > benvida.txt
```

Súbeo ao HDFS:
```bash
hdfs dfs -put benvida.txt /user/hadoop/
```

Comproba que se copiou correctamente:
```bash
hdfs dfs -ls /user/hadoop/
```
Tamén podemos probar a descargar algún arquivo mais realista:
`https://drive.google.com/uc?id=1NW7EnwxuY6RpMIxOazRVibOYrZfMjsb2&export=download`

- Copiámolo ao `namenode`:
```bash
docker cp people-100000.csv namenode:/tmp
```
- Dende o contedor subímolo ao cartafol `/datos`:
```bash
hdfs dfs -put /tmp/people-100000.csv /datos
```
---

### Paso 3: Visualizar e descargar ficheiros
Para ler un ficheiro directamente desde HDFS:
```bash
hdfs dfs -cat /user/hadoop/benvida.txt
```

Para descargalo ao sistema local:
```bash
hdfs dfs -get /user/hadoop/benvida.txt ./benvida_descargado.txt
```

---

### Paso 4: Copiar, mover e eliminar
```bash
hdfs dfs -cp /user/hadoop/benvida.txt /datos/copia.txt
hdfs dfs -mv /datos/copia.txt /datos/benvida_final.txt
hdfs dfs -rm /user/hadoop/benvida.txt
```

---

### Paso 5: Comprobar o uso do espazo
Consulta o espazo ocupado e dispoñible:
```bash
hdfs dfs -du -h /
hdfs dfs -df -h /
```

---

### Paso 6: Verificar a integridade dos datos
Usa o comando `fsck` para comprobar o estado do sistema de ficheiros:

```bash
hdfs fsck /datos -files -blocks -locations
```

---

### Paso 7: Administrar o modo seguro
O modo seguro impide cambios mentres o NameNode arranca ou realiza operacións críticas.

```bash
hdfs dfsadmin -safemode get
hdfs dfsadmin -safemode enter
hdfs dfsadmin -safemode leave
```

---

### Paso 8: Crear e restaurar un snapshot
Activa snapshots para un directorio:

```bash
hdfs dfsadmin -allowSnapshot /datos
hdfs dfs -createSnapshot /datos snapshot_inicial
```

Elimina ou restaura cando sexa necesario:
```bash
hdfs dfs -deleteSnapshot /datos snapshot_inicial
```

---

### Paso 9: Obter información do clúster
```bash
hdfs dfsadmin -report
hdfs dfsadmin -printTopology
```

---

### Paso 10: Balanceo e mantemento
Para equilibrar o espazo entre DataNodes:
```bash
hdfs balancer -threshold 10
```

E para realizar tarefas de mantemento:
```bash
hdfs namenode -verify
hdfs datanode -regular
```

---

### Retos propostos
1. Crea un script bash que automatice a subida dun conxunto de ficheiros locais a HDFS.  
2. Programa unha tarefa cron que execute `hdfs fsck` cada día e garde o resultado nun log.  
3. Explora o directorio `/tmp` de HDFS e elimina os ficheiros antigos.  
4. Proba a crear un snapshot e restaurar un ficheiro eliminado.

---

### Recursos recomendados
- [Guía oficial de HDFS](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)  
- [HDFS Design Documentation](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)  
- [Cloudera – HDFS Overview](https://docs.cloudera.com/cdp-private-cloud-base/7.1.7/hdfs-overview/topics/hdfs-introduction.html)  
- [IBM Developer – Intro to HDFS](https://developer.ibm.com/articles/os-cn-hadoop-hdfs/)
