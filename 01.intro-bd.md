---
layout: default
title: Introduci√≥n ao Big Data
---

# Introduci√≥n **Big Data**

---
## 1. Que √© o Big Data?

**Big Data** ref√≠rese ao conxunto de datos que, polo seu **volume**, **variedade** e **velocidade** de xeraci√≥n, superan as capacidades das ferramentas tradicionais de procesamento e an√°lise.

Non se trata s√≥ da cantidade de datos, sen√≥n de como xestionalos, procesalos e extraer valor deles.

### As 5 V's do Big Data
A√≠nda que ao principio eran 3, agora consid√©rase que estas son as 5 caracter√≠sticas fundamentais que definen un sistema *Big Data*:

| V              | Significado                                                                          |
| -------------- | ------------------------------------------------------------------------------------ |
| **Volume**     | Grandes cantidades de datos (terabytes, petabytes‚Ä¶).                                 |
| **Velocidade** | Xeraci√≥n e procesamento de datos en tempo real ou case real.                         |
| **Variedade**  | Datos estruturados, semiestruturados (XML, JSON) e non estruturados (v√≠deo, texto‚Ä¶). |
| **Veracidade** | Fiabilidade, calidade e consistencia dos datos.                                      |
| **Valor**      | Capacidade de obter co√±ecemento √∫til ou predici√≥ns a partir dos datos.               |
| **Viabilidade** | Capacidade da organizaci√≥n para realizar un uso eficaz dos datos |
| **Visualizaci√≥n**      | Capacidade de representar os datos visualmente mediante gr√°ficos ou indicaderos (KPI) para que sean lexibles e accesibles.               |

### A orixe do Big Data
Estas son as raz√≥ns que impulsaron o nacemento do **Big Data**:
- A dixitalizaci√≥n masiva (m√≥biles, sensores, redes sociais...).
- A necesidade de tomar decisi√≥ns baseadas en datos (data-driven).
- A aparici√≥n de tecnolox√≠as "baratas" de almacenamento e procesamento distribu√≠do.

### Arquitectura t√≠pica dun sistema Big Data
1. **Inxesta de datos**: Captaci√≥n de datos dende m√∫ltiples fontes: webes, sensores, APIs...).
2. **Almacenamento**: Gardar datos nun formato escalable, normalmente distribu√≠do (*HDFS*, *S3*, *Delta Lake*...).
3. **Procesamento**: Limpeza, transformaci√≥n e an√°lise de datos (*Apache Spark*, *Apache Beam*, *Apache Flink*...).
4. **Visualizaci√≥n**: Creaci√≥n de informes e dashboards (*Power BI*, *Tableau*, *Superset*...).
5. **Gobernanza/orquestraci√≥n**: Control de acceso, seguridade, calidade de datos (*Apache Airflow*, *Apache Nifi*, *Ozie*...).
![Arquitectura](images/arquitectura-bd.png)
### Retos do Big Data
A d√≠a de hoxe, estes son os principais retos do **Big Data**:
- Escalabilidade e rendemento.
- Garantir a calidade dos datos.
- Protexer a privacidade e a seguridade.
- Xestionar a complexidade das infraestruturas.
- Interpretar correctamente os resultados.
![Os retos do Big Data](images/big-data-cloud-based-solution.webp)
## 2. Contexto: profesi√≥ns e perf√≠s relacionados cos datos

A xesti√≥n de grandes volumes de datos non √© un fin en si mesmo, sen√≥n un medio para obter informaci√≥n, tomar decisi√≥ns mellores ou automatizar procesos. Neste contexto existen distintos **perf√≠s profesionais** que traballan con datos:

### Analista de datos (*Data Analyst*)
- Usa ferramentas como Excel, SQL ou Power BI para responder preguntas de negocio.
- Explora, visualiza e comunica resultados.
- Traballa sobre datos xa limpos e preparados.

### Enxe√±eiro/a de datos (*Data Engineer*)
- Constr√∫e a infraestrutura para que os datos poidan ser almacenados, procesados e consumidos.
- Usa ferramentas como Spark, Kafka, Airflow, HDFS...
- Enc√°rgase da calidade, integridade e dispo√±ibilidade dos datos.

### Cient√≠fico/a de datos (*Data Scientist*)
- Aplica modelos estat√≠sticos ou de *machine learning* para descubrir patr√≥ns ou facer predici√≥ns.
- Usa Python, R, notebooks, bibliotecas de IA.
- Precisa datos preparados previamente pola enxe√±ar√≠a de datos.

### Arquitecto de datos
- Define a infraestrutura necesaria para a xesti√≥n de grandes volumes de datos.
- Enc√°rgase de definir a estratexia respecto √° escalabilidade, gobernanza, linaxe e seguridade dos datos.

> Neste m√≥dulo imos traballar sobre todo as tarefas propias da **enxe√±ar√≠a de datos**, pero tocando tam√©n parte do traballo anal√≠tico.

---
## 3. Enxe√±er√≠a de datos
√â a base sobre a que se sustentan a ciencia e a anal√≠tica de datos en produci√≥n.

A **enxe√±ar√≠a de datos** trata do movemento, manipulaci√≥n e xesti√≥n dos datos. Se buscamos unha definici√≥n m√°is completa, podemos dicir que se centra no desenvolvemento, implementaci√≥n e mantemento dos sistemas e procesos que recuperan os datos en cru e producen informaci√≥n consistente e de alta calidade que d√° soporte aos diferentes casos de uso, como poden ser a anal√≠tica de datos ou a aprendizaxe autom√°tica (*machine learning*).

**Enxe√±eiro de datos**: Encargado de xestionar o cliclo de vida, recuperando os datos dende os sistemas de orixen e serv√≠ndoos a futuros consumidores, que poden ser cient√≠ficos de datos, ferramentas de visualizaci√≥n, modelos de IA, etc.

Poden distinguirse os seguintes roles:
### Roles de enxe√±ar√≠a de datos

- **Enxe√±eiro/a de datos de produto**: encargado/a de instalar, configurar e manter os produtos do equipo de enxe√±ar√≠a de datos, como poden ser **Airflow, Kafka ou Spark**.  

- **Enxe√±eiro/a de datos de *pipeline***: encargado/a de traballar co fluxo de datos, con co√±ecementos de **Python, SQL, Spark**, as√≠ como de traballar con **lagos de datos ou data lakehouse**.  

- **Enxe√±eiro/a de datos de BI**: con co√±ecementos de **SQL** e ferramentas de visualizaci√≥n como **Power BI ou Tableau**, para mostrar anal√≠ticas.  

### Cliclo de vida dos datos
1. **Xeraci√≥n**: quen, onde, como e cando se crean os datos: ficheiros en diferentes formatos, APIs, bases de datos OLTP, sistemas OLAP, etc.
2. **Almacenamento**: Estruturado e non estruturado. Verase con detalle pr√≥ximamente.
3. **Consumo**: O destino dos datos adoita ser an√°lise, aprendizaxe m√°quina ou ETLs inversas.
![ciclo de vida dos datos](images/05de-ciclo-vida.png)

### Almacenamento
Ainda que xa profundizaremos noutras unidades, antes de comezar √© necesario ter unha idea das diferentes capas que se te√±en en contar no almacenamento masivo de datos:
- **Capa f√≠sica**: Ref√≠rese aos medios de almacenamento: HDD, SDD, RAM, etc.
- **Sistemas de almacenamento**: Capa que se sit√∫a por riba da f√≠sica. Alg√∫ns exemplos son os sistemas de ficheiros distribu√≠dos (HDFS), Os Sistemas de xesti√≥n de bases de datos relacionais (Posgres, Mysql, etc.), sistemas de xesti√≥n de bases de datos NoSQL (MongoDB, Cassandra, ElasticSearch,etc.), almacenamento de obxectos (S3), etc.
- **Abstraci√≥n de almacenamento**: Capa superior que aporta organizaci√≥n dos datos mecanismos de consulta, caracter√≠sticas especiais (*Time Travel*, etc.). Poden estar enfocadas a datos estruturados (*Data Warehouse*, *Olap*, etc.), semiestruturados (*DataLake*, *Delta Lake*) ou ambos (*LakeHouse*).
![ciclo de vida dos datos](images/05storage.png)

Poden distinguirse as seguintes fases:
#### Inxesta
Consiste en mover datos dende unha fonte ao almacenamento. Conceptos fundamentais:
- **pipeline** de datos.
- Estratexias de inxesta: *push*, *pull* ou *poll*.
- Inxesta baseada en *vent√°s temporais* ou *cantidade*.
- Inxesta mediante *snapshots* ou *incremental*.
- Inxesta dende ficheiros.
- *ETL* vs *ELT*.

#### Transformaci√≥n

A fase de **transformaci√≥n** incl√∫e:

- **Consultas**  
  - Analizar plans de execuci√≥n e co√±ecer o optimizador de cada motor.  
  - Evitar `joins` complexos, mellor persistir resultados intermedios en t√°boas.  
  - Usar **CTEs** en vez de subconsultas.  
  - Evitar escaneos completos de t√°boas/columnas ‚Üí filtrar, limitar e usar √≠ndices.  
  - En datos en *streaming*, co√±ecer tipos de fiestras (vent√°s) e marcas de auga.  

- **Modelado de datos**  
  - Introducir a l√≥xica de negocio mediante modelos conceptuais, l√≥xicos e f√≠sicos.  
  - Normalizaci√≥n/denormalizaci√≥n segundo sexa preciso.  
  - Co√±ecer modelado multidimensional:  
    - **Kimball**: esquema en estrela (t√°boa de feitos + dimensi√≥ns).  
    - **Data Vault**: *hubs*, enlaces e sat√©lites.  

- **Transformaci√≥ns**  
  - Unir resultados de consultas e modelos para xerar valor.  
  - Consultas complexas poden persistirse temporal ou permanentemente.  
  - Prec√≠sase un sistema de **orquestraci√≥n**.  
  - Ferramentas: SQL (Hive, Spark SQL) ou c√≥digo (Pandas, PySpark).  
  - Co√±ecer limitaci√≥ns de sistemas *insert-only* (reescritura de ficheiros, truncado e recarga, rexistros con versi√≥n temporal, etc.).  
  - Persistencia mediante t√°boas, vistas ou vistas materializadas.  
  - Posible uso de **consultas federadas** (combinar datos de varios sistemas).  


---

#### Serving (Presentaci√≥n dos datos)

Unha vez transformados, os datos pres√©ntanse para **anal√≠tica** ou **ML**, asegurando sempre a s√∫a validaci√≥n.

- **Anal√≠tica de datos**  
  - **De negocio**: uso de datos hist√≥ricos e actuais para tomar decisi√≥ns (dashboards, KPIs).  
  - **Operacional**: reacci√≥n inmediata con datos recentes.  
  - **Embebida**: anal√≠tica integrada en aplicaci√≥ns (ex. monitorizaci√≥n en AWS EC2).  

- **Machine Learning**  
  - Calidade dos datos = calidade do modelo.  
  - Conceptos clave:  
    - Supervisado / non supervisado / semisupervisado.  
    - Clasificaci√≥n vs regresi√≥n.  
    - Tratamento de series temporais.  
    - Modelos cl√°sicos (regresi√≥n, √°rbores, SVM) vs deep learning.  
    - AutoML vs dese√±o manual de modelos.  
  - Todos os datos deben converterse a num√©ricos: *feature engineering*, codificaci√≥n categ√≥rica, embeddings.  
  - Elecci√≥n de hardware: CPU, GPU, local ou cloud segundo problema e tama√±o do dataset.  

- **Formas de entrega de datos a ML**  
  - Ficheiros (dependendo do uso e acceso).  
  - Bases de datos (*warehouses*, *lakehouses*).  
  - Sistemas de *streaming*.  
  - Consultas federadas (con menor rendemento).  
  - **Jupyter Notebooks**: experimentaci√≥n local ou en cloud, logo migraci√≥n a scripts.  
  - **Reverse ETL**: reintroducir datos transformados na fonte de orixe (ex. inserir audios etiquetados nun proxecto).  

### Destrezas e √°mbitos de apoio na enxe√±ar√≠a de datos

Ademais das fases principais, existen unha serie de √°mbitos transversais (*undercurrents*) que sustentan a enxe√±ar√≠a de datos:

#### Seguridade
- Principio de menor privilexio (PoLP) para persoas e sistemas.  
- Control de tempos de acceso.  
- Protecci√≥n da visibilidade mediante encriptaci√≥n, enmascaramento, ofuscaci√≥n ou sistemas de acceso robustos.  
- A seguridade √© un √°mbito amplo en si mesmo ‚Üí conexi√≥n coa ciberseguridade.  

#### Xesti√≥n do dato
- **Gobernanza**: asegurar calidade, integridade, seguridade e usabilidade.  
- **Descubribilidade**: acceso, orixe, relaci√≥ns e significado dos datos mediante metadatos.  
- **Modelado**: relacional, NoSQL, JSON (APIs REST), GraphQL.  
- **Linaxe**: trazar o ciclo de vida do dato.  
- **Integraci√≥n e interoperabilidade**: orquestraci√≥n de procesos.  
- **√âtica e privacidade**: protecci√≥n de datos sensibles e cumprimento legal.  

#### DataOps
Conxunto de t√©cnicas e patr√≥ns para entregar valor r√°pido con datos de alta calidade.  
Elementos principais:
- **Automatizaci√≥n**: despregues (ex. DAGs en Airflow, dependencias en Python, CI/CD).  
- **Monitorizaci√≥n e observabilidade**: detecci√≥n temper√° de problemas.  
- **Xesti√≥n de incidentes**: resoluci√≥n autom√°tica ou rollback a versi√≥n estable.  

#### Arquitectura e orquestraci√≥n
- Arquitecturas: microservizos, big data, IoT, etc.  
- **Orquestraci√≥n**: coordinar inxesti√≥n, transformaci√≥n e almacenamento (ex. Airflow).  

#### Enxe√±ar√≠a do software
- T√©cnicas de **testing**.  
- Uso de funci√≥ns xanela para datos en *streaming*.  
- Noci√≥ns de **DevOps**: infraestrutura como c√≥digo.  
- Control de versi√≥ns e boas pr√°cticas de desenvolvemento.  

---

### Obxectivos dun enxe√±eiro/a de datos
- Optimizar o **retorno do investimento (ROI)**.  
- Reducir **custos** (financeiros e de oportunidade).  
- Minimizar **riscos** (seguridade, calidade dos datos).  
- Maximizar o **valor e utilidade dos datos**.  

### Tecnolox√≠as esenciais para un/ha enxe√±eiro/a de datos

Ademais das **soft skills**, un bo enxe√±eiro/a de datos deber√≠a ter destreza, como m√≠nimo, nas seguintes tecnolox√≠as:

- **Linux** (especialmente bash) ‚Üí para operaci√≥ns no sistema operativo.  
- **SSH e redes** ‚Üí acceso a m√°quinas remotas.  
- **APIs REST** ‚Üí inxesti√≥n de datos de fontes externas.  
- **Git, GitHub e GitHub Actions** ‚Üí co√±ecemento do ciclo de **CI/CD**.  
- **Docker** ‚Üí xesti√≥n de contedores para despregar ferramentas.  
- **Jupyter Notebooks** ‚Üí desenvolver procesos de extracci√≥n, transformaci√≥n e carga de datos (ETL).  
- **SQL** ‚Üí lingua franca para a transformaci√≥n e anal√≠tica de datos.  
- **Linguaxes de programaci√≥n**:  
  - **Python**: fundamental para scripts e programas de soporte.  
  - **Scala**: relevante nalg√∫ns √°mbitos da enxe√±ar√≠a de datos.  


## 4. Tecnolox√≠as que imos empregar (e alternativas por categor√≠a)

### Almacenamento distribu√≠do
**Ferramentas do curso**:
- **HDFS** ‚Üí Sistema de ficheiros distribu√≠do de Hadoop, dese√±ado para traballar con grandes volumes de datos e tolerancia a fallos. √ösase como base en moitas arquitecturas Big Data.
- **MinIO** ‚Üí Soluci√≥n de almacenamento obxectual, lixeira e compatible coa API de Amazon S3. Ideal para despregues en contornas locais ou privadas.

**Outras alternativas**:
- **Amazon S3**, **Azure Data Lake**, **Google Cloud Storage** ‚Üí opci√≥ns na nube, altamente escalables.
- **Ceph** ‚Üí almacenamento distribu√≠do de c√≥digo aberto, usado en contornas h√≠bridas.

---

### Procesamento de datos
**Ferramenta principal**:
- **Apache Spark (PySpark)** ‚Üí Motor de procesamento distribu√≠do. Permite traballar en **batch**, **streaming** en tempo real, consultas SQL, machine learning (MLlib) e gr√°ficos (GraphX).

**Alternativas co√±ecidas**:
- **Apache Flink** ‚Üí moi potente en *streaming* de baixa latencia.
- **Apache Beam** ‚Üí modelo unificado para batch e streaming (exec√∫tase sobre Spark, Flink, Dataflow).
- **Hive, Presto/Trino** ‚Üí SQL distribu√≠do sobre grandes volumes de datos.
- **Dask** ‚Üí alternativa en Python para computaci√≥n distribu√≠da, m√°is sinxela ca Spark en certos escenarios.

---

### Inxesti√≥n e fluxo de datos
**Ferramentas empregadas**:
- **Apache NiFi** ‚Üí Interface gr√°fica para dese√±ar fluxos de datos, con conectores a m√∫ltiples sistemas.
- **Apache Airflow** ‚Üí Orquestrador e planificador de tarefas. Moi usado para *ETL pipelines* e integraci√≥n con Python.

**Alternativas populares**:
- **Talend** e **StreamSets** ‚Üí ferramentas gr√°ficas comerciais e open source de ETL.
- **Dagster** ‚Üí alternativa moderna a Airflow, centrada en *data pipelines* robustos.
- **Azure Data Factory**, **AWS Glue** ‚Üí servizos de nube para integraci√≥n e orquestraci√≥n.

---

### Comunicaci√≥n entre sistemas
**Ferramenta opcional e avanzada**:
- **Apache Kafka** ‚Üí Plataforma distribu√≠da de *streaming*. Permite integraci√≥n en arquitecturas orientadas a eventos, con alta escalabilidade e tolerancia a fallos.

**Outras opci√≥ns**:
- **RabbitMQ** ‚Üí cola de mensaxes lixeira, boa para microservizos.
- **Apache Pulsar** ‚Üí alternativa moderna a Kafka, con mellor integraci√≥n en multi-cluster.
- **MQTT** ‚Üí protocolo moi usado en IoT por ser lixeiro.
- **Azure EventHub**, **Google Pub/Sub** ‚Üí soluci√≥ns na nube para *event streaming*.

---

### Visualizaci√≥n e exploraci√≥n de datos
**Ferramentas que imos usar**:
- **Apache Superset** / **Metabase** ‚Üí Plataformas open source para crear **dashboards interactivos** e consultas SQL.
- **JupyterLab** ‚Üí Entorno flexible en Python, √∫til para an√°lise exploratoria, estat√≠stica e visualizaci√≥n con Pandas, Matplotlib, Seaborn, etc.

**Alternativas comerciais**:
- **Power BI**, **Tableau**, **Looker**, **QuickSight** ‚Üí Ferramentas de Business Intelligence con m√°is soporte empresarial, integraci√≥n directa con m√∫ltiples fontes de datos e opci√≥ns avanzadas de visualizaci√≥n.

---

---

## 5. D√∫as opci√≥ns arquitect√≥nicas: soluci√≥n comercial ou personalizada

| Opci√≥n                           | Caracter√≠sticas                                                  | Vantaxes                                                | Riscos ou custos                                    |
|----------------------------------|------------------------------------------------------------------|---------------------------------------------------------|-----------------------------------------------------|
| **Soluci√≥n comercial integral** | Plataforma unificada na nube con servizos integrados             | Sinxeleza, rapidez de posta en marcha, rendemento alto | Custos por uso, dependencia do provedor             |
| **Arquitectura personalizada**  | Montaxe combinada con ferramentas libres e autoaloxadas          | Control total, aprendizaxe profunda, custo baixo       | Complexidade inicial, mantemento                   |

> üîß No curso imos constru√≠r unha **arquitectura personalizada**, que permita ao alumnado:
> - Entender cada compo√±ente tecnol√≥xico.
> - Adaptarse m√°is tarde a plataformas comerciais como Databricks, Fabric ou Confluent.

---

## 6. Soluci√≥ns comerciais end-to-end: descrici√≥n e relaci√≥n coas tecnolox√≠as

### Databricks
- Plataforma cloud creada polos fundadores de Apache Spark.
- Baseada nun modelo *Lakehouse*: datos en bruto e refinados no mesmo sistema.
- Incorpora: Spark, Delta Lake, clusters autoxestionados, notebooks, MLflow, orquestraci√≥n.
- **Tecnolox√≠as similares vistas no curso**: Spark, MinIO/HDFS, Airflow, Superset.

---

### Microsoft Fabric
- Plataforma unificada de datos dentro do ecosistema Microsoft 365.
- Incl√∫e: OneLake, Data Factory, Power BI, Notebooks, Spark engine.
- **Tecnolox√≠as similares vistas no curso**: MinIO, NiFi, Airflow, Spark, Superset.

---

### Google Cloud Platform
- Ecosistema baseado en BigQuery (DWH), Dataflow (Apache Beam), Looker (BI).
- Incorpora tam√©n Pub/Sub (streaming).
- **Tecnolox√≠as similares vistas no curso**: Spark (substitu√≠do por Beam), Kafka, Superset, Airflow.

---

### Amazon Web Services
- Plataforma modular: S3 (almacenamento), Glue (ETL), Redshift (DWH), QuickSight (BI).
- Incl√∫e MSK (Kafka xestionado).
- **Tecnolox√≠as similares vistas no curso**: MinIO, NiFi, Airflow, Spark, Kafka, Superset.

---

### Confluent Platform
- Versi√≥n empresarial de Apache Kafka.
- Incorpora: Kafka Connect, KSQL, Schema Registry, Control Center.
- Foco en arquitecturas orientadas a eventos en tempo real.
- **Tecnolox√≠as similares vistas no curso**: Kafka, NiFi, Airflow.

---

### T√°boa comparativa

| Plataforma        | Almacenamento | Procesamento | Inxesti√≥n / ETL      | Visualizaci√≥n BI | Streaming / eventos |
|------------------|----------------|--------------|-----------------------|------------------|----------------------|
| Databricks       | ADLS / S3      | ‚úî Spark      | ‚úî Pipelines propios   | ‚úî Dashboards     | ‚úî Kafka integrado    |
| MS Fabric        | OneLake        | ‚úî Spark      | ‚úî Data Factory        | ‚úî Power BI       | EventHub (parcial)   |
| GCP              | GCS            | ‚úî Beam       | ‚úî Dataflow            | ‚úî Looker         | ‚úî Pub/Sub            |
| AWS              | S3             | ‚úî Spark      | ‚úî Glue                | ‚úî QuickSight     | ‚úî MSK (Kafka)        |
| Confluent        | Externo (S3‚Ä¶)  | ‚úñ            | ‚úî Kafka Connect       | Opcional         | ‚úî Kafka completo     |

---

## 7. Ferramentas e contidos que se van desenvolver no curso

| Categor√≠a              | Ferramentas ou contidos inclu√≠dos                          | Obxectivo no curso                                                 |
|------------------------|-------------------------------------------------------------|---------------------------------------------------------------------|
| Almacenamento          | HDFS, MinIO                                                  | Comprender almacenamento distribu√≠do e obxectual                   |
| Procesamento batch     | Apache Spark + PySpark                                       | ETL e an√°lises distribu√≠das con RDDs e DataFrames                  |
| Procesamento streaming | Spark Structured Streaming + fontes reais (Kafka ou APIs)   | An√°lise de datos en tempo real                                     |
| Inxesti√≥n / ETL        | Apache NiFi, Airflow                                         | Automatizaci√≥n de extracci√≥n e carga de datos                     |
| Comunicaci√≥n           | Apache Kafka (teor√≠a e pr√°ctica opcional)                   | Sistemas orientados a eventos, logs distribu√≠dos                   |
| Visualizaci√≥n          | Superset / Metabase, JupyterLab                             | Dashboards e an√°lise exploratoria                                  |
| Orquestraci√≥n          | Airflow, NiFi                                                | Dependencias, planificaci√≥n e control de execuci√≥n de pipelines    |
| BI / an√°lise final     | Power BI (observaci√≥n), Superset (uso pr√°ctico)              | Comparativa real entre alternativas libres e comerciais             |

## 8. Glosario de termos
### Almacenamento
- **Data lake**: Repositorio de almacenamento masivo onde se gardan datos en bruto en calquera formato (non estruturados e semi-estruturados).
- **Data Warehouse**: Almac√©n de datos estruturados, dese√±ado para a an√°lise e a toma de decisi√≥ns. Habitualmente con topolox√≠a en estrela.
- **Delta Lake**: Capa transaccional ACID sobre un data lake que garante calidade, consistencia e control
- **Delta Table**:	T√°boa en formato Delta con capacidades avanzadas de consulta e modificaci√≥n de datos
### Procesamento de datos
- **Batch Processing**: Procesamento de datos en bloques ou lotes, con latencia alta pero bo rendemento para grandes volumes.
- **Streaming Processing**: Procesamento en tempo real de fluxos de datos continuamente xerados.
- **ETL (Extract, Transform, Load)**: Proceso cl√°sico de extracci√≥n, transformaci√≥n e carga de datos.
- **ELT (Extract, Load, Transform)**: Variante moderna onde os datos se cargan primeiro e logo se transforman no destino.
### An√°lise e ciencia de datos
- **DataFrame**: Estrutura de datos tabular usada en Spark e Pandas.
- **Data Profiling**: T√©cnica para analizar a estrutura, calidade e estat√≠sticas dos datos.
- **Feature Engineering**: Proceso de transformaci√≥n de datos en caracter√≠sticas relevantes para modelos de aprendizaxe autom√°tica.
- **Model Deployment**: Fase de despregue dun modelo para que poida ser usado en produci√≥n.
### Visualizaci√≥n e ciencia de datos
- **Dashboard**: Panel visual que presenta indicadores clave e m√©tricas en tempo real.
- **KPIs (Key Performance Indicators)**: M√©tricas cr√≠ticas que indican o rendemento dos procesos de negocio.
### Calidade, monitorizaci√≥n e seguridade
- **Data Lineage**: Seguimento da orixe e transformaci√≥n dos datos ao longo do seu ciclo de vida.
- **Observabilidade de datos**: Capacidade de rastrear, monitorizar e alertar sobre problemas nos fluxos de datos.
- **Data Governance**: Pol√≠ticas e pr√°cticas para asegurar o uso responsable, seguro e legal dos datos.
- **DLP (Data Loss Prevention)**: Conxunto de ferramentas para evitar a fuga de informaci√≥n sensible.
### Arquitectura
- **Lambda Architecture**: Modelo que combina procesamento batch e streaming para unha an√°lise m√°is completa.
- **Kappa Architecture**: Variante que usa s√≥ fluxos de datos en tempo real, simplificando o dese√±o.
- **Medallion Architecture**: Arquitectura por capas (bronze, silver, gold) para mellorar a calidade dos datos progresivamente.
### Outros conceptos
- **Metadata**: Datos que describen outros datos (estructura, tipo, fonte...).
- **Schema Evolution**: Capacidade dun sistema para adaptar cambios no esquema dos datos.
- **Partitioning**: T√©cnica para dividir grandes conxuntos de datos en subconxuntos m√°is pequenos para mellorar o rendemento.
- **Shuffling**: Redistribuci√≥n de datos entre tarefas no proceso de agrupaci√≥n ou combinaci√≥n en Spark.
