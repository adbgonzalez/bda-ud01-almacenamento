{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UD1 \u2013 Bloque 7 e 8: Exercicios pr\u00e1cticos con Delta Lake e Apache Iceberg\n",
        "\n",
        "Este notebook recolle exercicios guiados para repetir e probar os exemplos dos apuntamentos:\n",
        "\n",
        "- UD1 \u2013 Bloque 7: Delta Lake sobre HDFS e S3 (MinIO)\n",
        "- UD1 \u2013 Bloque 8: Apache Iceberg sobre HDFS e S3 (MinIO)\n",
        "\n",
        "O obxectivo \u00e9 que poidas:\n",
        "\n",
        "- Crear e consultar t\u00e1boas Delta e Iceberg en HDFS e MinIO.\n",
        "- Practicar operaci\u00f3ns ACID: inserci\u00f3n, actualizaci\u00f3n, *time travel*.\n",
        "- Ver como cambian os ficheiros f\u00edsicos e os metadatos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Preparaci\u00f3n da sesi\u00f3n de Spark\n",
        "\n",
        "No noso cl\u00faster, a sesi\u00f3n de Spark adoita estar xa creada como `spark`.\n",
        "Se non \u00e9 as\u00ed, crea unha nova sesi\u00f3n co seguinte c\u00f3digo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Parte 1 \u2013 Delta Lake sobre HDFS\n",
        "\n",
        "Nesta parte imos reproducir o **Escenario 1** do Bloque 7:\n",
        "creaci\u00f3n dunha t\u00e1boa Delta en HDFS, lectura, consulta, MERGE e *time travel*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Crear un DataFrame de exemplo\n",
        "\n",
        "Pequeno conxunto de datos en memoria con lecturas de temperatura."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "data = [\n",
        "    (\"2024-01-01\", \"A Coru\u00f1a\", 11.2),\n",
        "    (\"2024-01-01\", \"Vigo\", 13.5),\n",
        "    (\"2024-01-02\", \"A Coru\u00f1a\", 9.8),\n",
        "    (\"2024-01-02\", \"Vigo\", 14.1)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"data\", \"cidade\", \"temperatura\"])\n",
        "df.show()\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Gardar como Delta Table en HDFS\n",
        "\n",
        "Escribimos o DataFrame en formato Delta en HDFS, na ruta `/datalake/meteo_delta`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "delta_path = \"/datalake/meteo_delta\"\n",
        "\n",
        "(df.write\n",
        " .format(\"delta\")\n",
        " .mode(\"overwrite\")\n",
        " .save(delta_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comproba o contido en HDFS co seguinte comando (desde o contedor do namenode):\n",
        "\n",
        "```bash\n",
        "hdfs dfs -ls /datalake/meteo_delta\n",
        "hdfs dfs -ls /datalake/meteo_delta/_delta_log\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Lectura e consulta da t\u00e1boa Delta\n",
        "\n",
        "Lemos os datos dende HDFS e facemos unha consulta SQL de agregaci\u00f3n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_delta = spark.read.format(\"delta\").load(delta_path)\n",
        "df_delta.show()\n",
        "\n",
        "df_delta.createOrReplaceTempView(\"meteo_delta\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT cidade, AVG(temperatura) AS media\n",
        "    FROM meteo_delta\n",
        "    GROUP BY cidade\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Actualizaci\u00f3n (upsert) con MERGE INTO\n",
        "\n",
        "Usamos `DeltaTable.forPath` para cargar a t\u00e1boa e facemos un MERGE\n",
        "que actualiza un rexistro e insire outro novo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_table = DeltaTable.forPath(spark, delta_path)\n",
        "\n",
        "novos_datos = [\n",
        "    (\"2024-01-02\", \"A Coru\u00f1a\", 10.2),  # actualizaci\u00f3n\n",
        "    (\"2024-01-03\", \"Ourense\", 6.4)     # inserci\u00f3n\n",
        "]\n",
        "\n",
        "df_novos = spark.createDataFrame(novos_datos, [\"data\", \"cidade\", \"temperatura\"])\n",
        "\n",
        "(delta_table.alias(\"old\")\n",
        " .merge(\n",
        "     df_novos.alias(\"new\"),\n",
        "     \"old.data = new.data AND old.cidade = new.cidade\"\n",
        " )\n",
        " .whenMatchedUpdateAll()\n",
        " .whenNotMatchedInsertAll()\n",
        " .execute())\n",
        "\n",
        "spark.read.format(\"delta\").load(delta_path).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Versionado e *time travel* en Delta Lake\n",
        "\n",
        "Cada escritura ou MERGE crea unha nova versi\u00f3n. Podemos ver o historial e ler versi\u00f3ns antigas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Historial de versi\u00f3ns\n",
        "spark.sql(f\"\"\"\n",
        "    DESCRIBE HISTORY delta.`{delta_path}`\n",
        "\"\"\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Ler a versi\u00f3n inicial (version 0)\n",
        "df_v0 = (spark.read.format(\"delta\")\n",
        "         .option(\"versionAsOf\", 0)\n",
        "         .load(delta_path))\n",
        "\n",
        "print(\"Versi\u00f3n 0 da t\u00e1boa:\")\n",
        "df_v0.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Parte 2 \u2013 Delta Lake sobre MinIO (S3A)\n",
        "\n",
        "Agora repetimos un escenario similar, pero gardando os datos nun bucket de MinIO usando o *filesystem* `s3a://`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Gardar e ler datos Delta en MinIO\n",
        "\n",
        "Neste exemplo asumimos que existe un bucket chamado `deltalake`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "delta_s3_path = \"s3a://deltalake/meteo_delta\"\n",
        "\n",
        "# Gardar no bucket de MinIO\n",
        "(df.write\n",
        " .format(\"delta\")\n",
        " .mode(\"overwrite\")\n",
        " .save(delta_s3_path))\n",
        "\n",
        "# Ler desde MinIO\n",
        "df_s3 = spark.read.format(\"delta\").load(delta_s3_path)\n",
        "df_s3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podes comprobar o contido dende:\n",
        "\n",
        "- A li\u00f1a de comandos de Hadoop:\n",
        "  ```bash\n",
        "  hadoop fs -ls s3a://deltalake/meteo_delta\n",
        "  ```\n",
        "- A consola web de MinIO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Parte 3 \u2013 Apache Iceberg sobre HDFS\n",
        "\n",
        "Nesta parte imos reproducir os exemplos principais do Bloque 8, usando o cat\u00e1logo `local`\n",
        "configurado para traballar con Iceberg en HDFS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Crear un namespace en Iceberg (cat\u00e1logo local)\n",
        "\n",
        "Creamos un namespace chamado `meteo` no cat\u00e1logo `local`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS local.meteo\")\n",
        "spark.sql(\"SHOW NAMESPACES IN local\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Importar o CSV `meteo.csv` como DataFrame\n",
        "\n",
        "Usamos o ficheiro `meteo.csv` gardado en `/home/hadoop/work/meteo.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "meteo_csv_path = \"/home/hadoop/work/meteo.csv\"\n",
        "\n",
        "df_meteo = (spark.read\n",
        "            .option(\"header\", True)\n",
        "            .option(\"inferSchema\", True)\n",
        "            .csv(meteo_csv_path))\n",
        "\n",
        "df_meteo.show()\n",
        "df_meteo.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Crear unha t\u00e1boa Iceberg en HDFS a partir do DataFrame\n",
        "\n",
        "Creamos a t\u00e1boa `local.meteo.lecturas` usando Iceberg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "(df_meteo.writeTo(\"local.meteo.lecturas\")\n",
        " .using(\"iceberg\")\n",
        " .create())\n",
        "\n",
        "spark.sql(\"SELECT * FROM local.meteo.lecturas\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Consultas SQL sobre a t\u00e1boa Iceberg\n",
        "\n",
        "Facemos unha agregaci\u00f3n simple por cidade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT cidade, AVG(temperatura) AS media\n",
        "    FROM local.meteo.lecturas\n",
        "    GROUP BY cidade\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Evoluci\u00f3n de esquema en Iceberg\n",
        "\n",
        "Engadimos unha nova columna chamada `humidade`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.sql(\"ALTER TABLE local.meteo.lecturas ADD COLUMN humidade DOUBLE\")\n",
        "spark.sql(\"DESCRIBE TABLE local.meteo.lecturas\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.6 Inserci\u00f3n de datos en Iceberg (tipos estritos)\n",
        "\n",
        "Iceberg \u00e9 estrito co tipo DATE, polo que usamos `DATE('YYYY-MM-DD')`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "    INSERT INTO local.meteo.lecturas\n",
        "    VALUES (DATE('2024-01-03'), 'Lugo', 7.3, 65.0)\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM local.meteo.lecturas WHERE cidade = 'Lugo'\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.7 Time travel en Iceberg\n",
        "\n",
        "Primeiro listamos os snapshots dispo\u00f1ibles, e despois usamos un `snapshot_id`\n",
        "para consultar o estado pasado da t\u00e1boa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "history_df = spark.sql(\"\"\"\n",
        "    SELECT snapshot_id, parent_id, operation, made_current_at\n",
        "    FROM local.meteo.lecturas.history\n",
        "\"\"\")\n",
        "history_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Substit\u00fae este valor por un snapshot_id real visto na cela anterior\n",
        "snapshot_id_exemplo = 0  # EXEMPLO: cambia este n\u00famero por un snapshot_id v\u00e1lido\n",
        "\n",
        "if snapshot_id_exemplo != 0:\n",
        "    df_old = spark.sql(f\"\"\"\n",
        "        SELECT *\n",
        "        FROM local.meteo.lecturas VERSION AS OF {snapshot_id_exemplo}\n",
        "    \"\"\")\n",
        "    df_old.show()\n",
        "else:\n",
        "    print(\"Actualiza 'snapshot_id_exemplo' cun valor v\u00e1lido antes de executar esta cela.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Parte 4 \u2013 Apache Iceberg sobre MinIO\n",
        "\n",
        "Agora imos crear unha t\u00e1boa Iceberg no cat\u00e1logo `minio`, que apunta a un warehouse en S3A/MinIO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Crear namespace en MinIO\n",
        "\n",
        "Creamos o namespace `meteo` dentro do cat\u00e1logo `minio`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS minio.meteo\")\n",
        "spark.sql(\"SHOW NAMESPACES IN minio\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Crear a t\u00e1boa Iceberg en MinIO\n",
        "\n",
        "Reutilizamos o DataFrame `df_meteo` e escrib\u00edmolo como t\u00e1boa Iceberg en MinIO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "(df_meteo.writeTo(\"minio.meteo.lecturas\")\n",
        " .using(\"iceberg\")\n",
        " .create())\n",
        "\n",
        "spark.sql(\"SELECT * FROM minio.meteo.lecturas\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Reescritura con particionamento din\u00e1mico\n",
        "\n",
        "Reescribimos os datos particionando por `cidade` para mellorar o rendemento en consultas filtradas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "    ALTER TABLE minio.meteo.lecturas\n",
        "    REWRITE DATA USING PARTITION BY (cidade)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Ver ficheiros f\u00edsicos e metadatos da t\u00e1boa en MinIO\n",
        "\n",
        "Listamos os ficheiros co\u00f1ecidos por Iceberg para esta t\u00e1boa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT * FROM minio.meteo.lecturas.files\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Exercicios adicionais propostos\n",
        "\n",
        "1. Modifica os exemplos para engadir m\u00e1is cidades e datas ao conxunto de datos inicial.\n",
        "2. Proba a engadir novas columnas tanto en Delta como en Iceberg e observa as diferenzas de comportamento.\n",
        "3. En Delta Lake, fai varias actualizaci\u00f3ns e comproba como cambian as versi\u00f3ns no `DESCRIBE HISTORY`.\n",
        "4. En Iceberg, crea outra t\u00e1boa con particionamento por `data` e compara o rendemento de consultas filtrando por rango de datas.\n",
        "5. Explora no HDFS e en MinIO a estrutura de directorios e ficheiros que crean Delta e Iceberg para as distintas t\u00e1boas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}