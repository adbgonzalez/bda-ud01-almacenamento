{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62c6a90",
   "metadata": {},
   "source": [
    "# UD1 – Bloques 7 e 8: Exercicios con Delta Lake e Apache Iceberg \n",
    "\n",
    "Este notebook recolle exercicios guiados para repetir os exemplos dos apuntamentos:\n",
    "\n",
    "- 07.Delta-Lake\n",
    "- 08.Apache-iceberg\n",
    "\n",
    "Empregaremos o dataset `penguins.csv` (por exemplo, o clásico *Palmer Penguins*), gardado en:\n",
    "\n",
    "```text\n",
    "/home/hadoop/work/data/penguins.csv\n",
    "```\n",
    "Hai que subilo a HDFS, por exemplo a `/data`\n",
    "Cada exercicio puntuará a puntuación indicada se está correcto e 0 en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6468386",
   "metadata": {},
   "source": [
    "## 0. Preparación da sesión de Spark\n",
    "\n",
    "Se a sesión de Spark non está creada, inicialízaa nesta cela.\n",
    "(Non puntúa, pero é necesaria para o resto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BoletinLakehouse\").getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12efdf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 1 – Delta Lake sobre HDFS\n",
    "\n",
    "Nesta parte imos crear e manexar unha táboa Delta almacenada en HDFS\n",
    "a partir do ficheiro `penguins.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e0e51",
   "metadata": {},
   "source": [
    "### Exercicio 1 (0,5 puntos) – Ler o dataset `penguins.csv` nun DataFrame\n",
    "\n",
    "Ler o ficheiro CSV con `header = True` e `inferSchema = True` e inspeccionar o esquema\n",
    "e algunhas filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484230ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_csv_path = \"/data/penguins.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0cede",
   "metadata": {},
   "source": [
    "### Exercicio 2 (0,5 puntos) – Gardar o DataFrame como táboa Delta en HDFS\n",
    "\n",
    "Garda o DataFrame `df` en formato Delta na ruta `/datalake/penguins_delta` (crea o directorio `datalake` en HDFS se non existe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path_hdfs = \"/datalake/penguins_delta\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e776677",
   "metadata": {},
   "source": [
    "Podes comprobar o contido en HDFS dende o contedor do namenode:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /datalake/penguins_delta\n",
    "hdfs dfs -ls /datalake/penguins_delta/_delta_log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a762b",
   "metadata": {},
   "source": [
    "### Exercicio 3 (0,75 puntos) – Lectura e consulta da táboa Delta\n",
    "\n",
    "Ler de novo a táboa Delta desde HDFS e facer unha consulta que devolva a media do `bill_length_mm` por especie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc5e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "201a2b9a",
   "metadata": {},
   "source": [
    "### Exercicio 4 (0,75 puntos) – Actualización con MERGE INTO\n",
    "\n",
    "Simula unha actualización de datos:\n",
    "- crea un pequeno DataFrame con algunhas novas observacións (por exemplo, \n",
    "  unha especie existente cun `bill_length_mm` lixeiramente distinto, e outra observación nova).\n",
    "- fai un `MERGE` contra a táboa Delta.\n",
    "\n",
    "Pista: podes usar como clave de combinación algunha combinación de columnas\n",
    "por exemplo (`species`, `island`). O obxectivo é practicar o MERGE,\n",
    "non tanto a corrección biolóxica dos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_path_hdfs)\n",
    "\n",
    "novas_obs = [\n",
    "    # posible actualización (mesma especie / illa)\n",
    "    (\"Adelie\", \"Torgersen\", 40.0, 18.7),\n",
    "    # posible rexistro novo\n",
    "    (\"Gentoo\", \"Biscoe\", 50.1, 15.3),\n",
    "]\n",
    "\n",
    "df_novos = spark.createDataFrame(\n",
    "    novas_obs,\n",
    "    [\"species\", \"island\", \"bill_length_mm\", \"bill_depth_mm\"]\n",
    ")\n",
    "\n",
    "# Complementamos co resto de columnas como nulas se non existen\n",
    "for col in df_delta.columns:\n",
    "    if col not in df_novos.columns:\n",
    "        df_novos = df_novos.withColumn(col, F.lit(None))\n",
    "\n",
    "df_novos = df_novos.select(df_delta.columns)  # mesma orde/columnas\n",
    "\n",
    "# Continuar aqui:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad203397",
   "metadata": {},
   "source": [
    "### Exercicio 5 (0,75 puntos) – Versionado e *time travel* en Delta Lake\n",
    "\n",
    "1. Usa `DESCRIBE HISTORY` para ver o historial da táboa.\n",
    "2. Le a versión inicial (por exemplo, `versionAsOf = 0`) e compáraa coa versión actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7188ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d61f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5be6a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 2 – Delta Lake sobre MinIO (S3A)\n",
    "\n",
    "Nesta parte imos repetir unha operación similar, pero gardando os datos nun bucket de MinIO,\n",
    "empregando o esquema `s3a://`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce25f2",
   "metadata": {},
   "source": [
    "### Exercicio 6 (0,5 puntos) – Gardar e ler a táboa Delta en MinIO\n",
    "\n",
    "1. Garda o DataFrame `df`  en `s3a://delta-lake/penguins_delta` (recorda crear o bucket `delta-lake` se non existe).\n",
    "2. Léeo de novo desde MinIO e comproba que os datos son accesibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path_s3 = \"s3a://delta-lake/penguins_delta\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc8461",
   "metadata": {},
   "source": [
    "Podes verificar o contido tamén con:\n",
    "\n",
    "```bash\n",
    "hadoop fs -ls s3a://delta-lake/penguins_delta\n",
    "```\n",
    "e na consola web de MinIO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48a98b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 3 – Apache Iceberg sobre HDFS (catálogo `local`)\n",
    "\n",
    "Agora imos usar Apache Iceberg sobre HDFS, empregando o catálogo `local`\n",
    "configurado no clúster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40863e",
   "metadata": {},
   "source": [
    "### Exercicio 7 (0,5 puntos) – Crear un namespace Iceberg para penguins\n",
    "\n",
    "Crea un namespace chamado `penguins` no catálogo `local` e comproba que aparece listado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cc10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53329bc2",
   "metadata": {},
   "source": [
    "### Exercicio 8 (0,75 puntos) – Crear unha táboa Iceberg a partir de `penguins.csv`\n",
    "\n",
    "Usa o DataFrame `df` lido de `penguins.csv` para crear a táboa `local.penguins.observacions`\n",
    "en formato Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45a165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b412747",
   "metadata": {},
   "source": [
    "### Exercicio 9 (0,75 puntos) – Consultas SQL sobre a táboa Iceberg\n",
    "\n",
    "Realiza as seguintes consultas:\n",
    "\n",
    "- número de rexistros por especie\n",
    "- media de `body_mass_g` por especie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3311d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84818db",
   "metadata": {},
   "source": [
    "### Exercicio 10 (0,75 puntos) – Evolución de esquema en Iceberg\n",
    "\n",
    "Engade unha columna nova chamada `notas` de tipo STRING á táboa Iceberg\n",
    "e comproba o novo esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f586fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f45fdbb5",
   "metadata": {},
   "source": [
    "### Exercicio 11 (0,5 puntos) – Inserción de datos respectando os tipos\n",
    "\n",
    "Insire algunha fila nova na táboa, tendo coidado co tipo das columnas.\n",
    "\n",
    "No caso de que o teu `penguins.csv` non teña datas, podes inserir deixando\n",
    "algunhas columnas a NULL ou adaptando á estrutura real do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966c25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "533f818b",
   "metadata": {},
   "source": [
    "### Exercicio 12 (0,75 puntos) – Time travel en Iceberg\n",
    "\n",
    "1. Lista os snapshots dispoñibles na táboa `local.penguins.observacions`.\n",
    "2. Elixe un `snapshot_id` e consulta o estado da táboa nese snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6cb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba9993",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 4 – Apache Iceberg sobre MinIO (catálogo `minio`)\n",
    "\n",
    "Por último, imos crear e manexar unha táboa Iceberg almacenada en MinIO,\n",
    "empregando o catálogo `minio` configurado sobre `s3a://spark/iceberg-warehouse`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5dd091",
   "metadata": {},
   "source": [
    "### Exercicio 13 0,75 puntos) – Crear namespace en `minio`\n",
    "\n",
    "Crea o namespace `penguins` no catálogo `minio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b04e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d60a07b",
   "metadata": {},
   "source": [
    "### Exercicio 14 (0,75 puntos) – Crear táboa Iceberg en MinIO a partir de `penguins.csv`\n",
    "\n",
    "Crea a táboa `minio.penguins.observacions` usando o DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946ff81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5588c588",
   "metadata": {},
   "source": [
    "### Exercicio 15 (0,75 puntos) – Inspeccionar ficheiros físicos en MinIO\n",
    "\n",
    "Lista os ficheiros coñecidos por Iceberg para esta táboa e observa como cambian\n",
    "despois da reescritura/particionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28ba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd7b7b-7766-4b00-8273-3d4dbdd34635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
