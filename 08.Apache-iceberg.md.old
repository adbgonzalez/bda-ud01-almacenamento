# üü¶ UD1 ‚Äì Bloque 8: Apache Iceberg sobre HDFS e S3 (MinIO)

## üü© Obxectivos da pr√°ctica
- Comprender que √© **Apache Iceberg** e o seu papel dentro dunha arquitectura Lakehouse.
- Comparar Iceberg con **Delta Lake**, entendendo vantaxes e limitaci√≥ns.
- Configurar o cl√∫ster Spark/Hadoop para usar Iceberg.
- Crear cat√°logos, namespaces e t√°boas Iceberg.
- Importar datasets existentes (CSV ‚Üí Iceberg).
- Practicar time travel, evoluci√≥n de esquema e optimizaci√≥n.
- Traballar con HDFS e MinIO.

## 1. Teor√≠a: de Data Lake a Iceberg
### 1.1 Limitaci√≥ns dos Data Lakes tradicionais
Os Data Lakes permiten almacenar grandes volumes de datos, pero carecen de:
- ACID
- Lecturas consistentes
- Evoluci√≥n de esquema robusta
- Xesti√≥n eficiente de ficheiros
- Rendemento consistente cando hai moitos ficheiros

### 1.2 Tecnolox√≠as Lakehouse
- **Delta Lake**
- **Apache Iceberg**
- **Apache Hudi**

### 1.3 Que √© Apache Iceberg?
Iceberg √© un formato de t√°boa transaccional dese√±ado para Data Lakes modernos.
Incl√∫e:
- ACID real
- Metadatos escalables (manifest files)
- Particionamento oculto
- Evoluci√≥n de esquema avanzada
- Time travel nativo
- Compatibilidade multisistema (Spark, Flink, Trino‚Ä¶)

### 1.4 Iceberg vs Delta Lake
| Caracter√≠stica | Delta Lake | Apache Iceberg |
|----------------|------------|----------------|
| Gobernanza | Databricks | ASF |
| Motores soportados | Principalmente Spark | Spark, Flink, Trino‚Ä¶ |
| Metadatos | Delta Log | Manifest Lists / Manifest Files |
| Particionamento | Hive-style | Particionamento oculto |
| Evoluci√≥n de esquema | Boa | Excelente |
| Uso recomendado | Spark puro | Ecosistemas heterox√©neos |

---

## 2. Configuraci√≥n do cl√∫ster para Iceberg
### 2.1 JAR necesario
Copiar a:
```
/opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.5.0.jar
```

### 2.2 Activar Iceberg en Spark
```
spark.sql.extensions org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
```

### 2.3 Cat√°logo Iceberg en HDFS
```
spark.sql.catalog.local org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.local.type hadoop
spark.sql.catalog.local.warehouse hdfs://namenode:9000/warehouse/iceberg
```

### 2.4 Cat√°logo Iceberg en MinIO
```
spark.sql.catalog.minio org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.minio.type hadoop
spark.sql.catalog.minio.warehouse s3a://spark/iceberg-warehouse
```

### 2.5 Configuraci√≥n S3A
```
spark.hadoop.fs.s3a.endpoint http://minio:9000
spark.hadoop.fs.s3a.access.key minioadmin
spark.hadoop.fs.s3a.secret.key minioadmin
spark.hadoop.fs.s3a.path.style.access true
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
```

---

## 3. Escenario 1 ‚Äî Iceberg sobre HDFS
### 3.1 Crear namespace
```sql
CREATE NAMESPACE IF NOT EXISTS local.meteo;
```

### 3.2 Importar CSV
```python
df = spark.read.option("header", True).option("inferSchema", True).csv("/home/hadoop/work/meteo.csv")
```

### 3.3 Crear t√°boa Iceberg
```python
df.writeTo("local.meteo.lecturas").using("iceberg").create()
```

### 3.4 Consultas SQL
```sql
SELECT cidade, AVG(temperatura) FROM local.meteo.lecturas GROUP BY cidade;
```

### 3.5 Evoluci√≥n de esquema
```sql
ALTER TABLE local.meteo.lecturas ADD COLUMN humidade DOUBLE;
```

### 3.6 Inserci√≥n
```sql
INSERT INTO local.meteo.lecturas VALUES ('2024-01-03','Lugo',7.3,65.0);
```

### 3.7 Time Travel
```sql
SELECT * FROM local.meteo.lecturas.history;
SELECT * FROM local.meteo.lecturas VERSION AS OF 1;
```

---

## 4. Escenario 2 ‚Äî Iceberg sobre MinIO
### 4.1 Crear namespace
```sql
CREATE NAMESPACE IF NOT EXISTS minio.meteo;
```

### 4.2 Crear t√°boa en MinIO
```python
df.writeTo("minio.meteo.lecturas").using("iceberg").create()
```

### 4.3 Particionamento din√°mico
```sql
ALTER TABLE minio.meteo.lecturas REWRITE DATA USING PARTITION BY (cidade);
```

### 4.4 Ver ficheiros f√≠sicos
```sql
SELECT * FROM minio.meteo.lecturas.files;
```

---

## 5. Caso pr√°ctico adicional
Simulaci√≥n de carga diaria:
```python
from pyspark.sql.functions import lit
df_hoxe = df.withColumn("data", lit("2024-01-04"))
df_hoxe.writeTo("local.meteo.lecturas").append()
```

---

## 6. Exercicios propostos
1. Crear t√°boa particionada en MinIO.
2. Simular varias cargas diarias.
3. Engadir e eliminar columnas analizando cambios en metadatos.
4. Comparar snapshots Iceberg vs Delta Lake.
5. Explorar manifest files en MinIO.

---
