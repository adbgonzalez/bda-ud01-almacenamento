# UD1 – Bloque 5: Delta Lake sobre HDFS e S3 (MinIO)

## Obxectivos da práctica

- Comprender o concepto de **Delta Lake** e o modelo *Lakehouse*.
- Engadir funcionalidades **ACID** e de **versionado** a un Data Lake existente.
- Crear e consultar táboas Delta sobre **HDFS** e **MinIO/S3**.
- Aplicar operacións de escritura, actualización e *time travel*.
- Avaliar as vantaxes fronte a formatos tradicionais (Parquet, CSV).

---

## 1. Teoría: de Data Lake a Lakehouse
### 1.1. Data Warehouse
O Data Warehouse (DW) é un repositorio de datos estruturado, pensado para:
- Informes empresariais, BI e analítica clásica.
- Consultas SQL moi optimizadas.
- Modelos estrela/copo de neve, datos limpos e transformados.

**Limitacións dun DW tradicional**
- Estrutura ríxida (schemas fixos).
- Custos elevados ao escalar.
- Non se adapta ben a datos brutos, semiestruturados ou non estruturados.
- Dependencia dun sistema de almacenamento interno (non distribuído horizontalmente como HDFS ou S3).

### 1.2. Data Lake: flexibilidade sen garantías
Un Data Lake (HDFS, S3/MinIO, Azure Data Lake…) almacena datos en bruto de calquera tipo:
- Estruturados (CSV, Parquet)
- Semiestruturados (JSON, Avro)
- Non estruturados (imaxes, logs, texto)

**Vantaxes**
- Moi barato e escalable horizontalmente.
- Permite gardar todo tipo de datos.
- Ideal para data science, machine learning, exploración e inxestión masiva.

**Inconvenientes**
- Non hai transaccións ACID
- Lecturas inconsistentes se hai procesos escribindo.
- Esquema débil ou inexistente: risco de datos corruptos ou incompatibles.
- Difícil actualización: os formatos por ficheiros non permiten updates ou deletes eficientes.
- Non hai control de versións nin time travel.
#### O problema dos Data Lakes tradicionais

Os Data Lakes tradicionais (HDFS, S3) permiten almacenar grandes volumes de datos,
pero non garantizan:
- Consistencia entre lecturas e escrituras.
- Control de versións.
- Evolución do esquema.
- Integridade ante procesos concorrentes.

Isto dificulta o mantemento e a fiabilidade dos fluxos de datos analíticos.
### 1.3. Lakehouse: o mellor de ambos mundos
O modelo **Lakehouse** busca combinar as características dos dous modelos anteriores:
| Data Lake      | Data Warehouse    |
| -------------- | ----------------- |
| Flexibilidade  | Consistencia      |
| Baixo custo    | consultas fiables |
| Escalabilidade | esquema forte     |
| Multi-formato  | transaccións ACID |

O obxectivo é que o data lake poida actuar como un DW sen renunciar ao almacenamento barato e distribuído.

Tecnoloxías Lakehouse populares:
- **Delta Lake** (open source)
- **Apache Hudi**
- **Apache Iceberg**
## 2. Delta Lake
### 2.1. Que é un Delta Lake?

**Delta Lake** é unha capa de almacenamento *open source* que se coloca enriba de HDFS ou S3/MinIO e engade:

| Funcionalidade | Descrición |
|----------------|-------------|
| **ACID transactions** | Escrituras e actualizacións atómicas e consistentes. |
| **Schema enforcement** | Impide introducir datos incompatibles co esquema. |
| **Time travel** | Permite consultar versións anteriores dos datos. |
| **Upserts e merges** | Actualización eficiente de rexistros existentes. |

Estrutura típica dunha táboa Delta:
```bash
/ruta/da/taboa/
   ├── part-00000-...snappy.parquet
   ├── part-00001-...snappy.parquet
   └── _delta_log/
       ├── 000000.json
       ├── 000001.json
       └── ...
```

### 2.2. Que ofrece Delta Lake?
1. **Transaccións ACID**
Cada operación de escritura/actualización crea unha nova entrada en _delta_log.
**ACID → Atomicity, Consistency, Isolation, Durability**
- *Atomicidade*: ou se aplica todo ou nada.
- *Consistencia*: o estado final é sempre válido.
- *Illamento*: lecturas coherentes aínda con varios procesos en paralelo.
- *Durabilidade*: o estado final non se perde tras fallos.

Grazas ao log transaccional, Spark sabe que ficheiros son válidos en cada versión.

2. **Schema enforcement & schema evolution**

Delta impide que se carguen datos erróneos:
- **Schema enforcement** → valida datos antes de escribir.
- **Schema evolution** → permite actualizar o esquema (engadir columnas, por exemplo).

3. **Time Travel**

Delta permite consultar calquera versión pasada:
```sql
SELECT * FROM taboa VERSION AS OF 3;
SELECT * FROM taboa TIMESTAMP AS OF '2025-01-01';
```

Isto é fundamental para:
- Auditorías
- Reproducibilidade
- Depuración de procesos ETL
- Comparación de estados pasados

4. **Upserts / MERGE INTO**

Unha necesidade común no mundo real:
- Actualizar rexistros existentes
- Inserir novos
- Eliminar duplicados
- Facer SCD Type 2

Delta facilita isto:
```sql
MERGE INTO destino d
USING cambios c
ON d.id = c.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
```

En formatos como Parquet puro isto sería complexo e ineficiente.

5. Rendemento e optimización

Delta incorpora tecnoloxías de optimización:
- **Data skipping**
- **Z-Ordering** (para acelerar consultas de filtrado)
- **Compaction / OPTIMIZE**
- **Vacuum** para eliminar versións antigas
### 2.3. Arquitectura simplificada

┌───────────────────────────────┐
│            Spark SQL          │
└───────────────┬───────────────┘
                │   (Catalyst + DataFrame API)
                ▼
┌───────────────────────────────┐
│         Delta Engine          │
│  ─ transaccións ACID          │
│  ─ control de versións        │
│  ─ log JSON + checkpoints     │
│  ─ optimización de consultas  │
└───────────────┬───────────────┘
                │
                ▼
┌───────────────────────────────┐
│    HDFS / S3 (MinIO) + Parquet│
│    ─ datos en columnas        │
│    ─ alta compresión          │
│    ─ escalabilidade barata    │
└───────────────────────────────┘


---

## 3. Preparación do contorno

### Requisitos

- Clúster Spark operativo (por exemplo, [spark-cluster](https://github.com/adbgonzalez/spark-cluster))
- Hadoop/HDFS ou MinIO accesible.
- Python ≥ 3.8
- Paquete Delta Lake instalado.

### Iniciar unha sesión PySpark con Delta Lake

```bash
pyspark   --packages io.delta:delta-spark_2.12:3.1.0   --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"   --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
```

Se traballas dende Jupyter ou VS Code:
```python
from delta import configure_spark_with_delta_pip
from pyspark.sql import SparkSession

builder = (
    SparkSession.builder
    .appName("DeltaLakeDemo")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()
```

---

### 3.1. Escenario 1 — Delta Lake sobre HDFS

#### 3.1.1. Crear un DataFrame de exemplo

```python
# Pequeno conxunto de datos en memoria (lista de tuplas)
data = [
    ("2024-01-01", "A Coruña", 11.2),
    ("2024-01-01", "Vigo", 13.5),
    ("2024-01-02", "A Coruña", 9.8),
    ("2024-01-02", "Vigo", 14.1)
]

# Creamos un DataFrame de Spark con nomes de columnas
df = spark.createDataFrame(data, ["data", "cidade", "temperatura"])

# Visualizamos o contido
df.show()
```

#### 3.1.2. Gardar como Delta Table

```python
# Gardar en formato Delta dentro de HDFS
df.write.format("delta").mode("overwrite").save("/datalake/meteo_delta")
```

Comproba en HDFS:

```bash
hdfs dfs -ls /datalake/meteo_delta
hdfs dfs -ls /datalake/meteo_delta/_delta_log
```

#### 3.1.3. Lectura e consulta

```python
# Ler a táboa Delta dende HDFS
df_delta = spark.read.format("delta").load("/datalake/meteo_delta")

# Crear vista SQL temporal
df_delta.createOrReplaceTempView("meteo")

# Consulta SQL
spark.sql("""
    SELECT cidade, AVG(temperatura) AS media
    FROM meteo
    GROUP BY cidade
""").show()
```

#### 3.1.4. Actualización (upsert) con MERGE

```python
from delta.tables import DeltaTable

# Cargar a táboa Delta como DeltaTable
delta_table = DeltaTable.forPath(spark, "/datalake/meteo_delta")

# Novos datos: un rexistro actualiza outro existente, outro é novo
novos_datos = [
    ("2024-01-02", "A Coruña", 10.2),  # actualización
    ("2024-01-03", "Ourense", 6.4)     # inserción
]

df_novos = spark.createDataFrame(novos_datos, ["data", "cidade", "temperatura"])

# Upsert (MERGE)
(
    delta_table.alias("old")
    .merge(
        df_novos.alias("new"),
        "old.data = new.data AND old.cidade = new.cidade"
    )
    .whenMatchedUpdateAll()
    .whenNotMatchedInsertAll()
    .execute()
)
```

#### 3.1.5. Versionado e *time travel*

```python
# Ver historial de versións e operacións
spark.sql("""
    DESCRIBE HISTORY delta.`/datalake/meteo_delta`
""").show(truncate=False)

# Ler a versión inicial (version 0)
df_v0 = (
    spark.read.format("delta")
    .option("versionAsOf", 0)
    .load("/datalake/meteo_delta")
)

df_v0.show()
```

### 3.2. Escenario 2 — Delta Lake sobre MinIO (S3A)

#### 3.2.1. Configurar conexión a MinIO (Hadoop → S3A)

Editar o ficheiro `core-site.xml` no namenode:

```xml
<configuration>

  <property>
    <name>fs.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
  </property>

  <property>
    <name>fs.s3a.access.key</name>
    <value>admin</value>
  </property>

  <property>
    <name>fs.s3a.secret.key</name>
    <value>supersecret</value>
  </property>

  <property>
    <name>fs.s3a.endpoint</name>
    <value>http://minio:9000</value>
  </property>

  <property>
    <name>fs.s3a.path.style.access</name>
    <value>true</value>
  </property>

  <property>
    <name>fs.s3a.connection.ssl.enabled</name>
    <value>false</value>
  </property>

  <property>
    <name>fs.s3a.aws.credentials.provider</name>
    <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
  </property>

</configuration>
```

Verificar dende Hadoop:

```bash
hadoop fs -ls s3a://deltalake/
```

#### 3.2.2. Gardar e ler datos Delta en MinIO

```python
# Gardar DataFrame como Delta nun bucket de MinIO
df.write.format("delta").mode("overwrite").save("s3a://deltalake/meteo_delta")

# Ler de MinIO
df_s3 = spark.read.format("delta").load("s3a://deltalake/meteo_delta")
df_s3.show()
```

#### 3.2.3. Verificación na MinIO UI

Accede á consola web de MinIO (por exemplo: http://localhost:9001) e deberías ver:

```
deltalake/
 └── meteo_delta/
      ├── _delta_log/
      ├── part-*.parquet
      └── ...
```

### 3.3. Caso de uso adicional — Carga diaria con histórico e auditoría

```python
from delta.tables import DeltaTable

ruta_delta = "/datalake/meteo_diaria"

# 1) Crear táboa inicial
if not DeltaTable.isDeltaTable(spark, ruta_delta):
    df.write.format("delta").mode("overwrite").save(ruta_delta)

# 2) Novos datos dun día + corrección
datos_hoxe = [
    ("2024-01-02", "Vigo", 13.9),
    ("2024-01-03", "A Coruña", 7.3),
    ("2024-01-03", "Vigo", 8.1)
]
df_hoxe = spark.createDataFrame(datos_hoxe, ["data", "cidade", "temperatura"])

t = DeltaTable.forPath(spark, ruta_delta)

(
    t.alias("old")
    .merge(
        df_hoxe.alias("new"),
        "old.data = new.data AND old.cidade = new.cidade"
    )
    .whenMatchedUpdateAll()
    .whenNotMatchedInsertAll()
    .execute()
)

# 3) Mostrar versión actual
spark.read.format("delta").load(ruta_delta).show()

# 4) Historial
spark.sql(f"DESCRIBE HISTORY delta.`{ruta_delta}`").show(truncate=False)

# 5) Ler versión anterior
df_antes = (
    spark.read.format("delta")
    .option("versionAsOf", 0)
    .load(ruta_delta)
)
df_antes.show()
```